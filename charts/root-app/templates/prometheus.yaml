apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: prometheus
  finalizers:
  - resources-finalizer.argocd.argoproj.io
spec:
  project: default
  source:
    repoURL: https://prometheus-community.github.io/helm-charts
    chart: prometheus
    targetRevision: 25.3.1
    helm:
      valuesObject:
        pushgateway:
          enabled: false
        # Okta group to grafana role mapping. Every non-mapped group is Viewer
        roleMapping:
        - group: platform-engineering-element
          role: Admin
        - group: engineering-function
          role: Editor
        - group: information-security-function
          role: Editor
        - group: product-function
          role: Viewer
        - group: business-operations-market-operations-functional-team
          role: Viewer
        # Based on https://github.com/grafana/helm-charts/tree/main/charts/grafana
        replicas: 1
        podDisruptionBudget:
          maxUnavailable: 1
        ingress:
          enabled: true
          hosts:
          - grafana.internal.corp.traderepublic.com
          ingressClassName: nginx-internal
          tls:
          - hosts:
            - grafana.internal.corp.traderepublic.com
            secretName: wildcard.internal.corp.traderepublic.com-tls
        initChownData:
          enabled: false
        autoscaling:
          enabled: true
          minReplicas: 2
          maxReplicas: 5
          targetCPU: "60"
        sidecar:
          dashboards:
            enabled: true
            searchNamespace: ALL
            folderAnnotation: "grafana_dashboard_folder"
            provider:
              foldersFromFilesStructure: true
        serviceMonitor:
          enabled: true
        grafana.ini:
          users:
            viewers_can_edit: true
            editors_can_admin: true
          auth:
            disable_login_form: false
          auth.okta:
            enabled: true
            name: Okta
            icon: okta
            allow_signup: false
            client_id: 0oa7zpw95tG8rlDLW417
            scopes: openid profile email groups
            auth_url: https://traderepublic.okta.com/oauth2/v1/authorize
            token_url: https://traderepublic.okta.com/oauth2/v1/token
            api_url: https://traderepublic.okta.com/oauth2/v1/userinfo
            role_attribute_path: "{{ range $.Values.roleMapping }}contains(groups[*], '{{ .group }}') && '{{ .role }}' || {{ end }}'Viewer'"
            allow_assign_grafana_admin: true
          database:
            type: postgres
            host: grafana.cble635qdd8b.eu-central-1.rds.amazonaws.com:5432
            name: grafana
            user: grafana
            ssl_mode: require
          security:
            csrf_trusted_origins: grafana.teleport.internal.corp.traderepublic.com
            csrf_additional_headers: ""
          server:
            domain: grafana.teleport.internal.corp.traderepublic.com
            root_url: https://%(domain)s/
          dashboards:
            default_home_dashboard_path: "/tmp/dashboards/Generic Dashboard/generic-service-dashboard.json"
        envFromSecret: grafana-credentials
        datasources:
          datasources.yaml:
            apiVersion: 1
            datasources:
            - name: Loki
              type: loki
              url: "http://loki-loki-distributed-gateway.loki.svc.cluster.local:443"  # http on :443 because we TLS offload on the AWL NLB
              jsonData:
                httpHeaderName1: X-Scope-OrgID
              secureJsonData:
                httpHeaderValue1: 'standard'
            - name: Loki-legacy
              type: loki
              url: "http://loki-loki-distributed-gateway.loki.svc.cluster.local:443"
              jsonData:
                httpHeaderName1: X-Scope-OrgID
              secureJsonData:
                httpHeaderValue1: 'fake'
            - name: Loki-restricted
              type: loki
              url: "http://loki-loki-distributed-gateway.loki.svc.cluster.local:443"  # Will be going through the proxy later on
              jsonData:
                httpHeaderName1: X-Scope-OrgID
              secureJsonData:
                httpHeaderValue1: 'restricted'
            - name: Tempo
              type: tempo
              url: "http://tempo-query-frontend.tempo.svc.cluster.local:3100"
              access: proxy
            - name: Mimir
              type: prometheus
              url: "http://mimir-nginx.mimir.svc.cluster.local:443/prometheus"
              isDefault: true
              jsonData:
                prometheusType: Mimir
                cacheLevel: 'High'
                incrementalQuerying: yes
            - name: Mimir - Alertmanager
              type: alertmanager
              url: "http://mimir-nginx.mimir.svc.cluster.local:443"  # http on :443 because we TLS offload on the AWL NLB
              access: server
              jsonData:
                implementation: mimir
                handleGrafanaManagedAlerts: true
            - name: Corporate - Alertmanager
              type: alertmanager
              url: "http://alertmanager-operated.monitoring.svc.cluster.local:9093"
              access: server
              jsonData:
                implementation: prometheus
        plugins:
          - flant-statusmap-panel
  destination:
    server: https://kubernetes.default.svc
    namespace: default
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
